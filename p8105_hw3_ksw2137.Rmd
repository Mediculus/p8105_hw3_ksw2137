---
title: "P8105 Data Science I - Homework 3"
author: "Kevin S.W."
date: "10/3/2019"
output: github_document
---

```{r setup, include = FALSE, warning = FALSE, message = FALSE}

# global default settings for chunks
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.width = 10, 
                      fig.align = "center",
                      results = "asis"
                      )

# loaded packages; placed here to be able to load global settings
Packages <- c("tidyverse", "dplyr")
invisible(lapply(Packages, library, character.only = TRUE))



# global settings for color palettes
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# theme global setting for ggplot
theme_set(theme_minimal() + 
            theme(legend.position = "bottom") +
            theme(plot.title = element_text(hjust = 0.5, size = 12),
                  plot.subtitle = element_text(hjust = 0.5, size = 8))
          )

```

# Introduction
Homework for showing capability in data visualization and exploratory data analysis. Package loaded along with default, global settings include `tidyverse` & `dplyr`.

# Problem 1 - InstaCart Data Wrangling and Visualization

```{r instacart_data}

# Loads instacart data from source and tidying
library(p8105.datasets)
data("instacart")

instacart <- instacart %>%  
  janitor::clean_names() %>%                            # cleans loaded df to lower case and snake_case
  mutate(                                               # turns aisle, aisle_id, and department into factors
    aisle = as.factor(str_to_title(aisle)),
    aisle_id = as.factor(aisle_id),
    department = as.factor(str_to_title(department))
  )

```

The InstaCart dataset collects data on customers and the things they order from local groceries. Data that we're using are cleaned previously and is limited because it only contains data from `eval_set = train`; likely dataset used in model-building. The dataframe contains `r nrow(instacart)` observations from `r ncol(instacart)` categories including `eval_set`. All these variables are linked in some way. For example, these variables are likely of particular importance:

* `order_id`: order identifier; if a person starts an order, all the items inside will have the same `order_id`
* `product_id`: product identifier which links a value to certain products (e.g. 340 = cake, 255 = soda, etc)
* `user_id`: customer identifier; unique id for customers (Peanut's id = 1000, John's id = 13,...)
* `order_dow`: the day of the week on which the order was placed (Mon, Tues, Wed,...)
* `order_hour_of_day`: the hour of the day on which the order was placed (13:00, 21:00,...)
* `product_name`: name of product such as "Organic Garlic", "Spring Water", or "Xpolsive Pizza Baked Snack Cracker"
* `aisle_id`: aisle identifier that is linked to `aisle` as a unique identifier
* `department_id`: department identifier
* `aisle`: the name of the aisle such as fresh vegetables or yogurt
* `department`: the name of the department (canned goods, snacks, frozen, ...)

### Problem 1a - Graph of Top Aisle Categories 

```{r instacart_explore}

# modifies source df to further process for ggplot
aisle_item_count <- instacart %>%                   # assigned source df to a new one for wrangling
  select(aisle_id, aisle, product_name) %>%        # select relevant columns
  drop_na() %>%                                    # drops any NA
  group_by(aisle, aisle_id) %>%                    # group by aisle & aisle_id
  summarize(aisle_order_count = n()) %>%           # creates a new column that counts unique orders in each aisle
  arrange(desc(aisle_order_count))                 # re-arrange to decreasing order

```

The cleaned and filtered data for our aisle vs order count contains `r nrow(aisle_item_count)` unique aisle categories. `r pull(aisle_item_count, aisle)[pull(aisle_item_count, aisle_order_count) == max(pull(aisle_item_count, aisle_order_count))]` is/are the most ordered category with `r max(pull(aisle_item_count, aisle_order_count))` orders. We then make a graph below to illustrate the top `r nrow(filter(aisle_item_count, aisle_order_count > 10000))` most ordered categories. 

```{r aisle_order_plot, eval = TRUE}

# modifies aisle-counts and print out a plot of the "ranking"
aisle_item_count %>% 
  filter(aisle_order_count > 10000) %>%                          # filters to contain counts > 10,000
  
# make a plot of count vs aisle category
  ggplot(aes(x = reorder(aisle, aisle_order_count),              # reorders aisle (factor) by using count as ref
             y = aisle_order_count, 
             fill = -aisle_order_count)                          # set fill based on count in decreasing order
         ) +
  geom_bar(stat = "identity") +                                  # bar plot with factors in x-axis
  labs(title = "Number of Items Ordered for Each Aisle",         # assigns labels
       subtitle = "for Items with > 10,000 orders",
       x = "Aisle Categories (from most to least)",
       y = "Number of Items Ordered"
       ) +
  coord_flip() +                                                 # flip x-y to better "stratify" categories
  theme(legend.position = "none",                                # remove legends
        axis.text.y = 
          element_text(hjust = 1, vjust = 0.5,                   # adjusts the hor/ver alignment of y-variables
                       size = 8, angle = -10,                    # resize text size and rotate at an angle
                       margin = margin(0, -80, 0, 70)),          # reduce margin to close gap between label/graph
        panel.grid.major.y = element_blank()                     # remove horizontal lines to improve readability
        )

```

The graph shows aisle categories with orders greater than 10,000 (e.g. butter-related items are ordered `r pull(aisle_item_count, aisle_order_count)[pull(aisle_item_count, aisle) == "Butter"]` times). Arranged from most to least, we can see a huge gap between the top ~5 versus the rest. In the graph, there are a total of `r nrow(filter(aisle_item_count, aisle_order_count > 10000))` items. 

### Problem 1b - Table for Top 3 Items in Particular Aisles

```{r insta_top3}

# filter df to show top 3 of most ordered
instacart %>% 
  select(aisle, product_name) %>%                 # pick relevant columns
  drop_na() %>%                                   # drop any NA
  group_by(aisle, product_name) %>%               # group by aisle and unique products within 
  summarize(product_count = n()) %>%              # make a new df that sums each unique product counts
  top_n(3, wt = product_count) %>%                # picks the top 3 from the product counts
  filter(aisle %in%                               # filter aisle to contain categories of interest
           c("Baking Ingredients", 
             "Dog Food Care", 
             "Packaged Vegetables Fruits")
         ) %>% 
  arrange(aisle, desc(product_count)) %>%         # arranges each aisle in decreasing order of product count
  rename("Aisle Category" = aisle,                # rename column variable to better present table
         "Product Name" = product_name,
         "Product Count" = product_count) %>%
  knitr::kable()                                  # prints table

```

We are also interested in the top 3 items in the "Baking Ingredients", "Dog Food Care", and "Packaged Vegetables Fruits" categories. The table above is a 9 x 3 grid; arranged first by aisle category in alphabetical order then by product count in decreasing order (from 1st - 3rd). Surprisingly packaged vegetables are, in general, much more popular compared to the other two. 

### Problem 1c - Table for Mean Hour-of-Day to Order Certain Products 

```{r mean_hour_table}

# filter original table to only show data for selected products
instacart %>% 
  select(order_dow, order_hour_of_day, product_name) %>%      # select relevant columns
  drop_na() %>%                                               # drop any NA
  group_by(product_name, order_dow) %>%                       # group by specified variables
  summarize(mean_hour = mean(order_hour_of_day)) %>%          # obtain the average hour for each product
  filter(product_name %in%                                    # filter product
           c("Pink Lady Apples", 
             "Coffee Ice Cream")
         ) %>% 
  mutate(order_dow =                                          # changes date from "1" to "Sun"
           lubridate::wday(order_dow + 1, label = T)          # used +1 to shift range from [0,6] to [1,7]
         ) %>% 
  rename("Product Name" = product_name) %>%                   # rename column name
  pivot_wider(names_from = order_dow,                         # change to wide form for easy comparison
              values_from = mean_hour
              ) %>% 
  knitr::kable(digits = 2)                                    # prints table and restricts to 2 decimal places

```

The table above shows the average hour of day that "Pink Lady Apples" and "Coffee Ice Cream" were ordered. Days were hardcoded as `0 - 6` and since it is a US company, I made the assumption that these start from Sun - Sat. The resulting table is a 2 x 7 table that gives the average hour these listed two items are ordered by day. Somewhat expected, the orders are typically done in the afternoon range. Ice cream orders are possibly (just a guess) due to the heat in the afternoon and thus acts as a "chiller" for consumers. 

# Problem 2 - Exploring and Visualizing BRFSS SMART Data
For this problem, we will be loading the BRFSS (Behavioral Risk Factors Surveillance System) data included in the p8105 package. As the data is huge and there were places where it's not tidy, we should tidy this up first by focusing on pertinent information (i.e. topic = "Overall Health" and only "Excellent" - "Poor" responses). 

```{r brfss_load}

# Loads BRFSS data from source
data("brfss_smart2010") 

brfss_smart2010 <- brfss_smart2010 %>%  
  janitor::clean_names() %>%                    # turns column variables to lower_snake_case
  rename(                                       # rename these variables to better describe the data inside
    state = locationabbr,
    county = locationdesc
    ) %>% 
  select(year, state, county, topic,            # select only relevant columns
         question, response, data_value) %>%   
  filter(topic == "Overall Health",             # filter df by topic of interest
         response %in%                          # and by specified survey response
           c("Excellent", "Very good", "Good", "Fair", "Poor")
         ) %>% 
  mutate(                                       # mutates the response variables to factor and re-order as well
    response = 
      factor(response,
             levels = c("Poor", "Fair", "Good", "Very good", "Excellent")),
    county = gsub(".*-", "", county),           # removes the state indicator in counties
    county = gsub(" County", "", county)        # removes the "county" since column title is county
    )
  
```

The cleaned data contains the key variables that we're interested to work with. They are:

* `year` : year in which the data were collected
* `state`:specific state where the responses come from (in 2-letter format) 
* `county`: counties within the states
* `topic`: topic in question; in this case "Overall Health" only
* `question`: questions being asked of respondents 
* `response`: responses from participants (poor, fair,  good, very good, excellent)
* `data_value`: the percent value of responses from each county (e.g. Jefferson County from AL has 18.9% "excellent" response)

### Problem 2a - Table of Number of State with >6 Participating Counties in 2002 vs 2010
In this problem, we will be filtering relevant data in order to make a table that lists the number of counties that participated from each state in the year 2002 vs 2010.

```{r brfss_count_table, results = "hold"}

# base df
brfss_count_loc <- brfss_smart2010 %>%                 # assigns a baseline, modified df for further cleaning
  select(year, state, county) %>%                      # selects relevant columns
  group_by(year, state) %>% 
  distinct(county) %>%                                 # removes duplicate counties since we only need uniques
  summarize(county_per_state = n())                    # counts up the number of counties in each state
  


# 2002 location count per state
brfss_count_loc %>%                                    # assigns a new df for 2002 counts
  filter(year == "2002",                               # filter by year 2002 and counts > 6
         county_per_state > 6
         ) %>%
  pivot_wider(names_from = state,                      # turn to wide form for kable()
              values_from = county_per_state
              ) %>% 
  rename("Year" = year) %>%                            # capitalizes "year" column name
  knitr::kable()                                       # Prints tables for 2002 States



# 2010 location counts per state
brfss_count_loc %>%                                    # assigns a new df for 2010 counts
  filter(year == "2010",                               # filter by year 2010 and counts > 6
         county_per_state > 6
         ) %>%
  pivot_wider(names_from = state,                      # turn to wide form for kable()
              values_from = county_per_state
              ) %>% 
  rename("Year" = year) %>%                            # capitalizes the "year" indicator
  knitr::kable()                                       # Prints tables for 2010

```

The resulting table above shows states with 7 or more participating counties in 2002 and 2010 respectively. Under each state is the number of counties within the state that responded. There are more than twice states that were part of the respondent in 2010 compared to 2002. We see that FL's counties increased significantly from `r pull(brfss_count_loc, county_per_state)[pull(brfss_count_loc, state) == "FL" & pull(brfss_count_loc, year) == "2002"]` to `r pull(brfss_count_loc, county_per_state)[pull(brfss_count_loc, state) == "FL" & pull(brfss_count_loc, year) == "2010"]`. We also do not see Connecticut (CT) in the 2010, likely because it had lower participating counties in 2010. Interestingly, while working on the table, I noticed that there are 49 states in the 2002 data but 51 states in 2010. After investigating, this is because Montana (MT) started participating in 2004 and Virginia (VA) in 2007. 

### Problem 2b - Fluctuations in "Excellent" Responses over Time by State 
In this problem, we are trying to collect data on how the average "excellent" responses fluctuate over time in each state. Since there are ~50 states, our graph will unfortunately look like a "spaghetti". 

```{r spaghetti_plot}

# filter out source data for plotting
brfss_smart2010 %>% 
  select(year, state, response, data_value) %>%                 # select relevant columns
  group_by(year, state) %>% 
  filter(response == "Excellent") %>%                           # filter to contain excellent responses only
  summarize(avg_data = mean(data_value, na.rm = TRUE)) %>%      # create a new df to include mean # of responses
  
# spaghetti plot for excellent responses
  ggplot(aes(x = year, y = avg_data, color = state)) +          # color category by state
  geom_line(size = 1, alpha = 0.8, na.rm = TRUE) +              # thicker line-size for increased visibility
  theme(legend.position = "right") +                            # legend position adjustment
  scale_x_discrete(limits = 2002:2010) +                        # adjust min, max of x-axis range
  guides(color = guide_legend(override.aes = list(size = 5))) + # resize the color code for legends
  labs(title = "Average Percentage of Excellent Responses",     # assigns labels
       subtitle = "Categorized by Participating States",
       x = "Year",
       y = "Percent of Excellent Response (%)",
       color = "State"
       )

```

The resulting line graph above does indeed look like a "spaghetti". However, based on the plot, we can definitely see certain "outliers". For example, there was a significant dip in the response in 2005. Based on our color legends, it is likely Wyoming (WY) or West Virginia (WV). Evaluating further, the state seems to be consistently on the very low end of excellent response. This might indicate that the overall health of the population there is less than excellent or the citizens might expect too much for an "excellent" health.

### Problem 2c - Distribution of Responses in NY Counties between 2006 and 2010
In this problem, we are interested in the distribution of each responses between the counties in New York (NY) between year 2006 and 2010. As such, we first need to filter our base data to contain the relevant variables.

```{r response_distr_plot, fig.height = 7, fig.width = 6}

brfss_smart2010 %>% 
  select(year, state, county, response, data_value) %>%           # selects relevant panels
  group_by(year, state, county) %>%                               # group by year, state, and county
  filter(state == "NY",                                           # filter to NY state only in 2006, 2010
         year %in% c("2006", "2010")) %>% 
  drop_na(response) %>%                                           # drop any NAs in responses
  
  ggplot(aes(x = response, y = data_value, fill = county)) +      # ggplot elements
  geom_bar(stat = "identity", position = "dodge") +               # identity = allow factor; dodge = side-by-side
  labs(title = "Percent of Responses to Overall Health Status",   # add titles
       subtitle = "in NYC State by Counties (Year 2006 & 2010)",
       x = "Overall Health Status Response",
       y = "Percent of Responses (%)",
       fill = " Counties"
       ) +
  facet_grid(year ~ .) +                                          # horizontal split by year
  theme(panel.grid.major.x = element_blank()                      # remove vertical lines in plot
        )

```

From the graph above, we can see that in both 2006 and 2010, most of the responses are clustered in the "Good" or "Very good". The distribution of poor to excellent on average doesn't seem to have changed much over the 4 years. However, we do see that there is an increase of 3 counties that were participating in 2010 (Bronx, Erie, and Kings). Further scrutiny shows us that New York and Nassau are consistently low in poor-good response and consequently high in very good - excellent responses.  

# Problem 3 - Accelerometer Data Wrangling and Visualization
In this problem we are dealing with the accelerometer dataset from an elderly man. We shall clean this so we could obtain pertinent information.

```{r accel_data}

accel_df <- read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>%                                    # cleans column title to lower_snake_case
  pivot_longer(activity_1:activity_1440,                        # turns activities into minutes
               names_to = "minute_mark",
               names_prefix = "activity_",
               values_to = "activity_count"
               ) %>% 
  mutate(minute_mark = as.numeric(minute_mark),                 # mutate minute_mark into numeric
         day = factor(day,                                      # changes days into factors
                      levels = c("Sunday", 
                                 "Monday", 
                                 "Tuesday",
                                 "Wednesday", 
                                 "Thursday",
                                 "Friday",
                                 "Saturday")),
         day_type = case_when(                                  # added conditions for labeling week-day/-end
           day == "Sunday" | day == "Saturday" ~ "Weekend",
           day != "Sunday" | day != "Saturday" ~ "Weekday"),
         day_type = factor(day_type)                            # changed the week-day/-end to factor
         ) %>% 
  select(week, day_id, day, day_type, everything())             # rearrange columns

```

After cleaning the data, we have a `r nrow(accel_df)` x `r ncol(accel_df)` table. The rows are basically cumulative minutes from 00:00 - 24:00 while the columns are:

* `week`: the week of month (week 1, 2, so on)
* `day_id`: the day of the month (day 1, 2, 3,...)
* `day`: day name that starts on Friday
* `day_type`: weekday vs weekend
* `minute_mark`: the cumulative minutes from 1 - 1440 (24 hours)
* `activity_count`: the amount of activity in the given minute

### Problem 3a - Total Daily Activity Table from Week 1 - 5 by Day of The Week
In this problem, we will be adding up all activity count for each day and create a table for the 35 days and see the total daily activity counts. 

```{r}

accel_df %>% 
  select(week, day_id, day, activity_count) %>%             # select relevant columns
  group_by(week, day) %>%                                   # group by week and day
  summarize(daily_activ_count = sum(activity_count)) %>%    # sums up the activity counts per day per week
  pivot_wider(names_from = day,                             # turn into reader-friendly form
              values_from = daily_activ_count
              ) %>%                                         
  rename("Week" = week) %>% 
  knitr::kable()                                            # prints the table

```

In the table above, we see that there is no real patterns to be seen if we go from week 1-5 day-by-day. However, if we look through each column of days, we see that this person's activities on Sunday has been decreasing consistently. His weekdays are generally very active with spikes in Monday - week 3 and Friday - week 5. Anomalies of much lower activities include Monday - week 1, Friday - week 4, Saturday - week 4, and Saturday - week 5. Since Saturday seem to have same numbers, this likely represents times where the person did not wear the tracker, which may also be the case for other, significantly low activities. 

### Problem 3b - Activity Tracking over 35 Days
In this problem we are interested to see the pattern of this patient's behavior for each day of the week. We will make an "activity" graph to hopefully find some patterns in his daily activities.

```{r daily_accel_plot}

accel_df %>% 
  ungroup() %>%                                                     # ungroup so select() works
  select(week, day_id, day, minute_mark, activity_count) %>%        # select relevant variables
  ggplot(aes(x = minute_mark/60, y = activity_count,                # make a 24-hour plot with days as factor
             fill = day)) +
  geom_point(aes(color = day), alpha = 0.7) +
  guides(color = guide_legend(override.aes = list(size = 5))) +
  theme(legend.position = "right") +                                # adjust legend position
  scale_x_discrete(limits = 0:24) +                                 # re-organize the x-labels
  labs(title = "24-hour Activity Count Over 35 Days",               # add labels to title, axes, etc
       subtitle = "Grouped by Day of The Week",
       x = "Time of Day (hours)",
       y = "Activity Count",
       fill = "",
       color = ""
       )

```

From the graph, we could see a gradual increase in activity count from 06:00 (likely waking up) with peaks at ~11:00 and ~21:00 primarily Fridays - Sundays. These are likely events where the person reaches the weekend and is going out for lunch/dinner outside home. Activities gradually declined again from ~21:00 which likely signifies the person going to sleep. However, similar to the spaghetti plot, this graph is too noisy to discern any further patterns. 