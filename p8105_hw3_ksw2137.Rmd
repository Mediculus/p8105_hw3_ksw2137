---
title: "P8105 Data Science I - Homework 3"
author: "Kevin S.W."
date: "10/3/2019"
output: github_document
---

```{r setup, include = FALSE, warning = FALSE, message = FALSE}

# global default settings for chunks
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.width = 10, 
                      fig.align = "center",
                      results = "asis"
                      )

# loaded packages; placed here to be able to load global settings
Packages <- c("tidyverse", "dplyr")
invisible(lapply(Packages, library, character.only = TRUE))

# global settings for color palettes
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# theme global setting for ggplot
theme_set(theme_minimal() + theme(legend.position = "bottom"))

```

# Introduction
Homework for showing capability in data visualization and exploratory data analysis. Package loaded along with default, global settings include `tidyverse` & `dplyr`.

# Problem 1 - InstaCart Data Wrangling and Visualization

```{r instacart_data}

# Loads instacart data from source
library(p8105.datasets)
data("instacart")


# Assigns a new dataframe to the data-source to prevent frequent "reloads"
### Not sure if necessary.
insta_df <- instacart %>%  
  janitor::clean_names() %>%                            # cleans loaded df to lower case and snake_case
  mutate(                                               # turns aisle, aisle_id, and department into factors
    aisle = as.factor(str_to_title(aisle)),
    aisle_id = as.factor(aisle_id),
    department = as.factor(str_to_title(department))
  )

```

The InstaCart dataset collects data on customers and the things they order from local groceries. Data that we're using are cleaned previously and is limited because it only contains data from `eval_set = train`; likely dataset used in model-building. The dataframe contains `r nrow(instacart)` observations from `r ncol(instacart)` categories including `eval_set`. All these variables are linked in some way. For example, these variables are likely of particular importance:

* `order_id`: order identifier; if a person starts an order, all the items inside will have the same `order_id`
* `product_id`: product identifier which links a value to certain products (e.g. 340 = cake, 255 = soda, etc)
* `user_id`: customer identifier; unique id for customers (Peanut's id = 1000, John's id = 13,...)
* `order_dow`: the day of the week on which the order was placed (Mon, Tues, Wed,...)
* `order_hour_of_day`: the hour of the day on which the order was placed (13:00, 21:00,...)
* `product_name`: name of product such as "Organic Garlic", "Spring Water", or "Xpolsive Pizza Baked Snack Cracker"
* `aisle_id`: aisle identifier that is linked to `aisle` as a unique identifier
* `department_id`: department identifier
* `aisle`: the name of the aisle such as fresh vegetables or yogurt
* `department`: the name of the department (canned goods, snacks, frozen, ...)

### Problem 1a - Making Graph for Top Aisle Categories 

This dataset is huge as indicated previously. As such, we have to filter out some things in the interest of efficiency. 

```{r instacart_explore}

# modifies source df to further process for ggplot
aisle_item_count <- insta_df %>%                   # assigned source df to a new one for wrangling
  select(product_name, aisle_id, aisle) %>%        # select relevant columns
  drop_na() %>%                                    # drops any NA
  group_by(aisle, aisle_id) %>%                    # group by aisle & aisle_id
  summarize(aisle_order_count = n()) %>%           # creates a new column that counts unique orders in each aisle
  arrange(desc(aisle_order_count))                 # re-arrange to decreasing order

```

The cleaned and filtered data for our aisle vs order count contains `r nrow(aisle_item_count)` unique aisle categories. `r pull(aisle_item_count, aisle)[pull(aisle_item_count, aisle_order_count) == max(pull(aisle_item_count, aisle_order_count))]` is/are the most ordered category with `r max(pull(aisle_item_count, aisle_order_count))` orders. We then make a graph below to illustrate the top `r nrow(filter(aisle_item_count, aisle_order_count > 10000))` most ordered categories. 

```{r aisle_order_plot, eval = TRUE}

# modifies aisle-counts and print out a plot of the "ranking"
aisle_item_count %>% 
  filter(aisle_order_count > 10000) %>%                          # filters to contain counts > 10,000
  
  ggplot(aes(x = reorder(aisle, aisle_order_count),              # make a plot of count vs aisle category
             y = aisle_order_count, 
             fill = -aisle_order_count)                          # set fill based on count in decreasing order
         ) +
  geom_bar(stat = "identity") +                                  # bar plot with factors in x-axis
  labs(title = "# of Items Ordered for Each Aisle",              # assigns labels
       subtitle = "for Items with > 10,000 orders",
       x = "Aisle Categories (from most to least)",
       y = "Number of Items Ordered"
       ) +
  coord_flip() +                                                 # flip x-y to better "stratify" categories
  theme(plot.title = element_text(hjust = 0.5, size = 12),       # adjusts title text size, alignment
        plot.subtitle = element_text(hjust = 0.5, size = 8),     # adjusts subtitle size, alignment
        legend.position = "none",                                # remove legends
        axis.text.y = 
          element_text(hjust = 1, vjust = 0.5,                   # adjusts the hor/ver alignment of y-variables
                       size = 8, angle = -10,                    # resize text size and rotate at an angle
                       margin = margin(0, -80, 0, 70)),          # reduce margin to close gap between label/graph
        panel.grid.major.y = element_blank()                     # remove horizontal lines to improve readability
        )

```

The graph shows aisle categories with orders greater than 10,000 (e.g. butter-related items are ordered `r pull(aisle_item_count, aisle_order_count)[pull(aisle_item_count, aisle) == "Butter"]` times). Arranged from most to least, we can see a huge gap between the top ~5 versus the rest. In the graph, there are a total of `r nrow(filter(aisle_item_count, aisle_order_count > 10000))` items. 

### Problem 1b - Table for Top 3 Items in Particular Aisles

We are also interested in the top 3 items in the "Baking Ingredients", "Dog Food Care", and "Packaged Vegetables Fruits" categories. Similarly, we need to filter our base dataframe to obtain these variables of interest.

```{r insta_top3}

# filter df to show top 3 of most ordered
insta_df %>% 
  select(product_name, aisle) %>%                 # pick relevant columns
  drop_na() %>%                                   # drop any NA
  group_by(aisle, product_name) %>%               # group by aisle and unique products within 
  summarize(product_count = n()) %>%              # make a new df that sums each unique product counts
  top_n(3, wt = product_count) %>%                # picks the top 3 from the product counts
  filter(aisle %in%                               # filter aisle to contain categories of interest
           c("Baking Ingredients", 
             "Dog Food Care", 
             "Packaged Vegetables Fruits")
         ) %>% 
  arrange(aisle, desc(product_count)) %>%         # arranges each aisle in decreasing order of product count
  rename("Aisle Category" = aisle,                # rename column variable to better present table
         "Product Name" = product_name,
         "Product Count" = product_count) %>%
  knitr::kable()                                  # prints table

```

The table above is a 9 x 3 grid; arranged first by aisle category in alphabetical order then by product count in decreasing order (from 1st - 3rd). Surprisingly packaged vegetables are, in general, much more popular compared to the other two. 

### Problem 1c - Table for Mean Hour-of-Day to Order Certain Products   
In this problem, we raise the question "what is the typical time that item x gets ordered in day y?". Particularly, we're interested in "Pink Lady Apples" and "Coffee Ice Cream". We will also split the time by day.

```{r mean_hour_table}

# filter original table to only show data for selected products
insta_df %>% 
  select(order_dow, order_hour_of_day, product_name) %>%      # select relevant columns
  drop_na() %>%                                               # drop any NA
  group_by(product_name, order_dow) %>%                       # group by specified variables
  summarize(mean_hour = mean(order_hour_of_day)) %>%          # obtain the average hour for each product
  filter(product_name %in%                                    # filter product
           c("Pink Lady Apples", 
             "Coffee Ice Cream")
         ) %>% 
  mutate(order_dow =                                          # changes date from "1" to "Sun"
           lubridate::wday(order_dow + 1, label = T)          # used +1 to shift range from [0,6] to [1,7]
         ) %>% 
  rename("Product Name" = product_name) %>%                   # rename column name
  pivot_wider(names_from = order_dow,                         # change to wide form for easy comparison
              values_from = mean_hour
              ) %>% 
  knitr::kable(digits = 2)                                    # prints table and restricts to 2 decimal places

```

The resulting table is a 2 x 7 table that gives the average hour these listed two items are ordered by day. Somewhat expected, the orders are typically done in the afternoon range.

# Problem 2

For this problem, we will be loading the BRFSS (Behavioral Risk Factors Surveillance System) data included in the p8105 package. As the data is huge and there were places where it's not tidy, we should tidy this up first by focusing on pertinent information (i.e. topic = "Overall Health and only "Excellent" - "Poor" responses). 

```{r brfss_load}

# Loads BRFSS data from source
data("brfss_smart2010")


# Assigns a new dataframe to the data-source to prevent frequent "reloads"
### Not sure if necessary.
brfss_df <- brfss_smart2010 %>%  
  janitor::clean_names() %>%             # turns column variables to lower_snake_case
  rename(                                # rename these variables to better describe the data inside
    state = locationabbr,
    county = locationdesc
    ) %>% 
  filter(topic == "Overall Health",      # filter df by topic of interest
         response %in%                   # and by specified survey response
           c("Excellent", 
             "Very good", 
             "Good", 
             "Fair", 
             "Poor")
         ) %>% 
  mutate(                                # mutates the response variables to factor and re-order as well
    response = 
      factor(response,
             levels = c("Poor",
                        "Fair",
                        "Good",
                        "Very good",
                        "Excellent"))
    )

```

### Problem 2a - Table of Number of State with >6 Participating Counties in 2002 vs 2010
In this problem, we will be filtering relevant data in order to make a table that lists the number of counties that participated from each state in the year 2002 vs 2010.

```{r brfss_count_table}

# base df
brfss_count_loc <- brfss_df %>%                        # assigns a baseline, modified df for further cleaning
  group_by(year, state) %>%                            # group by year and state
  select(year, state, county) %>%                      # selects relevant columns
  distinct(county) %>% 
  summarize(county_per_state = n())                    # counts up the number of counties in each state



# 2002 location count per state
count_loc_2002 <- brfss_count_loc %>%                  # assigns a new df for 2002 counts
  filter(year == "2002",                               # filter by year 2002 and counts > 6
         county_per_state > 6
         ) %>%
  pivot_wider(names_from = state,                      # turn to wide form for kable()
              values_from = county_per_state
              ) %>% 
  ungroup() %>%                                        # ungroup so select can remove year column
  select(-year)                                        # remove year column

# Prints tables for 2002 States
knitr::kable(count_loc_2002)



# 2010 location counts per state
count_loc_2010 <- brfss_count_loc %>%                  # assigns a new df for 2010 counts
  filter(year == "2010",                               # filter by year 2010 and counts > 6
         county_per_state > 6
         ) %>%
  pivot_wider(names_from = state,                      # turn to wide form for kable()
              values_from = county_per_state
              ) %>% 
  ungroup() %>%                                        # ungroup so select can remove year column
  select(-year)                                        # remove year column

# segmented tables for 2010
knitr::kable(count_loc_2010)

```

The resulting table contains `r ncol(count_loc_2002)` states and `r ncol(count_loc_2010)` states in 2002 and 2010 respectively. Interestingly, while working on the table, I noticed that there are 49 states in the 2002 data but 51 states in 2010. After investigating, this is because Montana (MT) started participating in 2004 and Virginia (VA) in 2007. 

### Problem 2b - Fluctuations in "Excellent" Responses over Time by State 

In this problem, we are trying to collect data on how the average "excellent" responses fluctuate over time in each state. Since there are ~50 states, our graph will unfortunately look like a "spaghetti". 

```{r spaghetti_plot}

# filter out source data for plotting
brfss_df %>% 
  select(response, year, state, data_value) %>%                 # select relevant columns
  group_by(year, state) %>%                                     # group by specified variables
  filter(response == "Excellent") %>%                           # filter to contain excellent responses only
  drop_na(data_value) %>%                                       # drop any NA (for states that didn't participate)
  summarize(avg_data = mean(data_value)) %>%                    # create a new df to include mean # of responses
  
# spaghetti plot for excellent responses
  ggplot(aes(x = year, y = avg_data, color = state)) +          # color category by state
  geom_line() +
  theme(legend.position = "right"                             # legend position adjustment
        #panel.background = element_rect(fill = "coral4")
        ) +
  scale_x_discrete(limits = 2002:2010) +                        # adjust min, max of x-axis range
  guides(color = guide_legend(override.aes = list(size = 6)))   # resize the color code for legends

```

The resulting line graph above does indeed look like a "spaghetti". However, based on the plot, we can definitely see a certain "outlier" in 2005. Based on our color legends, it is likely Wyoming (WY)

### Problem 2c - Making plot of responses in NY

```{r response_distr_plot, fig.height = 5}

test <- brfss_df %>% 
  select(response, year, state, data_value) %>% 
  group_by(year, state) %>% 
  filter(state == "NY",
         year %in% c("2006", "2010")) %>% 
  drop_na(response) %>% 
  
  ggplot(aes(x = response, y = data_value, fill = response)) +
  geom_bar(stat = "identity") +
  labs(title = "Number of Responses to Overall Health Status",
       subtitle = "in NYC State (Year 2006 & 2010)",
       x = "Overall Health Status",
       y = "# of Responses",
       fill = ""
       ) +
  facet_grid(. ~ year) +
  theme(plot.title = element_text(hjust = 0.5, size = 12),
        plot.subtitle = element_text(hjust = 0.5, size = 8),
        panel.grid.major.x = element_blank()
        )

```


# Problem 3

```{r accel_data}

accel_df <- read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(activity_1:activity_1440,
               names_to = "minute_mark",
               names_prefix = "activity_",
               values_to = "activity_count"
               ) %>% 
  mutate(minute_mark = as.numeric(minute_mark),
         day = factor(day, 
                      levels = c("Sunday", 
                                 "Monday", 
                                 "Tuesday",
                                 "Wednesday", 
                                 "Thursday",
                                 "Friday",
                                 "Saturday")),
         day_type = case_when(
           day == "Sunday" | day == "Saturday" ~ "Weekend",
           day != "Sunday" | day != "Saturday" ~ "Weekday"),
         day_type = factor(day_type)
         ) %>% 
  select(week, day_id, day, day_type, everything()) %>% 
  group_by(week, day_id)

```

Load, tidy, and otherwise wrangle the data. reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r}

daily_activ <- accel_df %>% 
  select(week, day_id, day, activity_count) %>% 
  summarize(daily_activ_count = sum(activity_count)) %>% 
  ungroup() %>% 
  select(-week) %>% 
  pivot_wider(names_from = day_id,
              values_from = daily_activ_count
              )

knitr::kable(
  list(wk1 = head(daily_activ)[1:7],
       wk2 = head(daily_activ)[8:14],
       wk3 = head(daily_activ)[15:21],
       wk4 = head(daily_activ)[22:28],
       wk5 = head(daily_activ)[29:35])
  )

```

### Problem 3b - Activity Tracking over 35 Days
In this problem we are interested to see the pattern of this patient's behavior 

Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r daily_accel_plot}

base_accel_plot <- accel_df %>% 
  ungroup() %>% 
  select(day_id, day, minute_mark, activity_count) %>% 
  group_by(day_id) %>% 
  ggplot(aes(x = minute_mark, y = activity_count, fill = day)) +
  geom_line() +
  #geom_bar(stat = "identity", alpha = 1) +
  guides(color = guide_legend(override.aes = list(size = 5))) +
  theme(legend.position = "right") +
  labs(title = "Activity Tracking over 35 Days",
       subtitle = "Grouped by Day of The Week",
       x = "Time of Day",
       y = "Activity Count",
       fill = ""
       )

base_accel_plot

```

The graph above definitely shows a typical pattern of 